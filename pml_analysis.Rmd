# -------------------------------------------------------------------- #
# ----            Coursera: Practical Machine Learning            ---- #
# -------------------------------------------------------------------- #

Author = J.Kromme
Date = 20-8-2014

This document is part of the Coursera course Practical Machine Learning hosted by John Hopkins Bloomberg School of Public health. Given by Jeff Leek, PhD, Roger D. Peng, PhD, Brian Caffo, PhD.

6 Persons performed barbell lifts correctly and incorrectly while wearing accelerometers on the belt, forarm, arm and dumbell. The dependent variable is a factor with five levels: A, B, C, D and E. The task for this assignment is to predict, using the given data, whether the barbell lift was done correctly or not. To evaluate the outcome of our analysis we got, besides a trainingset, a testset containing 20 cases. My result was 20 out of 20 correct! To evaluate within this assignment, I split the trainingset in a trainingset and a holdoutset (called: validationset).

Data was provides by the university, source:  http://groupware.les.inf.puc-rio.br/har.

First, load the need packages. 
```{r warning=FALSE}
# ---- Load packages ---- #
rm(list=ls())
set.seed(1234)
library(caret)
library(splines)
library(ggplot2)
library(rattle)
library(rpart)
library(rpart.plot)
library(klaR)
library(foreach)
library(doParallel)
library(randomForest)
```

The files are read and the trainingset is split into two set: trainingset and validationset. The files contained the values '#DIV/0!', these are excluded at reading in the data.

```{r}
# ---- Load data ---- #
# load data
# some '#DIV/0!' appears in data, delete those by reading in data
trainingset <- read.csv('./data/training.csv', na.strings = c('#DIV/0!', 'NA') )
testset <- read.csv('./data/test.csv', na.strings = c('#DIV/0!', 'NA') )

# partition train into train / validation set. to validate model as testset doesn't have a DV.
inTrain <- createDataPartition(trainingset[,1], p=0.7, list= FALSE)
trainingset <- trainingset[inTrain,]
validationset <-trainingset[-inTrain,]

# backup data
trainingsetBackup <- trainingset
testsetBackup     <- testset
validationsetBackup <- validationset
```


Data preparation is needed. First all columns where more then 60% of the values are missing are deleted. Also the first seven variables are deleted due to theoretical reasons, for example, you don't want to use an ID number of who is doing the execise as a predictor variable. Furthermore, I looked at NZV-variables, they did not exist. I also looked at histograms of all variables, I commented them out in this document otherwise there would be an overload of redundant graphs. But it did show some that some variables are highly skewed, or consists of two normal distributions. The featureplot confirmed this. This indicates non-linear effects.
```{r warning=FALSE}
# ---- Data preparation ---- #
#summary(trainingset)

# -- missing values
# so many missing values, imputing won't help. therefore delete these
toBeDeleted = list()
for (col in 1:ncol(trainingset)){
  # for each column, count number of missing values
  noOfMissingValues = length(trainingset[!complete.cases(trainingset[,col]),col])
  # if 40% or more is missing, delete variable from traing and testset
  if (noOfMissingValues > nrow(trainingset) * 0.4){
    
    cat('delete due to too many missing values:', names(trainingset)[col], '\n')
    toBeDeleted = c(toBeDeleted, names(trainingset)[col])
  }
}

trainingset <- trainingset[,!names(trainingset) %in% toBeDeleted]
testset <- testset[,!names(testset) %in% toBeDeleted]
validationset <- validationset[,!names(validationset) %in% toBeDeleted]


#summary(trainingset) # no missing values left

# check variable types
#str(trainingset) # correct

# delete due to theoretical reasons: row number and username etc won't explain useful variance. so exclude from model
trainingset <- trainingset[,-c(1:7)]
testset <- testset[,-c(1:7)]
validationset<- validationset[,-c(1:7)]

# delete variables with near zero variance
nearZeroVar(trainingset[,-53], saveMetrics = TRUE) # no variables with NZV

# plot histograms
for (col in 1:(ncol(trainingset) -1)){
  #hist(as.numeric(trainingset[,col]))
  if (col != ncol(trainingset) -1){
    #readline("Please press the Enter")
  }
} # few highly skewed, few with two normal distributions (1 - 4, 10), few with many zero variables

# investigate double distributions further
#featurePlot(x=trainingset[,c(1:4,10)], y = trainingset$classe, plot = 'pairs')

correlations <- abs(cor(trainingset[,c(1:4,10)]))
diag(correlations) <- 0
which(correlations > 0.7, arr.ind = T) # plots and correlation table shows high correlations -> need for PCA as preprocessing proces
```

The data preparation stage suggested non-linear effects, therefore no regression are used to model the data. Instead, a decision tree and a random forest model are trained. I expect the random forest to perform better than the decision tree. As random forest generally overfit less than decision trees. I expect random forest to perfom pretty well.

```{r warning=FALSE}
# ---- build models ---- #
# -- trees
modelFitTree <- train(trainingset$classe ~., method='rpart', data = trainingset[,-53])
finalModelTree <- modelFitTree$finalModel
modelFitTree
finalModelTree

fancyRpartPlot(finalModelTree)

confusionMatrix(trainingset$classe, predict(modelFitTree, newdata = trainingset))
confusionMatrix(validationset$classe, predict(modelFitTree, newdata = validationset))

# -- random forest - use randomForest package itself which includes combine function for doParallel
cl <- makePSOCKcluster(6)
registerDoParallel(cl)
modelFitRF <- foreach(ntree=rep(50, 6), .combine=randomForest::combine, .packages='randomForest') %dopar% {
 randomForest(trainingset[,-53], trainingset$classe, ntree=ntree)
 
}
stopCluster(cl)

confusionMatrix(trainingset$classe, predict(modelFitRF, newdata = trainingset))
confusionMatrix(validationset$classe, predict(modelFitRF, newdata = validationset))

varImpPlot(modelFitRF, n = 10)
```

The random forest model performs far better than the tree, therefore this model is used to predict the values in the testset. The values are already submitted and it scored a 20 out of 20 correct. roll_belt is by far the best predictor, followed by yaw_belt, pitch_forearm and magnet_dumbbell_z.


```{r warning=FALSE}
predict(modelFitRF, newdata = testset)

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(predict(modelFitRF, newdata = testset))

```